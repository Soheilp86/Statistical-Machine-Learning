{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqWKvuSfyZw0ewqCEPQwHF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Multiple Linear Regression II"],"metadata":{"id":"k33MTMuLBRxD"}},{"cell_type":"markdown","source":["When we perform multiple linear regression, we usually are interested in answering a few important questions."],"metadata":{"id":"XQjEQpc4Bayo"}},{"cell_type":"markdown","source":["## 1. Is atleast one of the predictor $X_1, X_2, \\dots, X_p$ helpful in predicting the response $y$?"],"metadata":{"id":"-hmdQz7_Y97l"}},{"cell_type":"markdown","source":["\n","\n","In simple linear regression, we check if there’s a relationship between the response and predictor by testing if the coefficient $\\beta_1$ is zero. For multiple regression with $p$ predictors, we test if all coefficients are zero ($\\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0$). This is done using an __F-statistic__. It compares the explained variance (due to the model) to the unexplained variance (due to residuals):\n","\n","$$\n","F = \\frac{(TSS - RSS) / p}{RSS / (n - p - 1)}\n","$$\n","\n","Where $n$ is the number of observations, $p$ is the number of predictors $TSS$ is the total sum of squares and $RSS$ is the residual sum of squares.\n","\n","\n","A large F-statistic indicates that at least one predictor is related to the response variable. But what counts as \"large\" depends on the number of observations $n$ and predictors $p$.\n","\n","- When $n$ is large: Even an F-statistic slightly greater than 1 might be significant.\n","- When $n$ is small: A larger F-statistic is needed to be significant.\n","\n","The F-statistic follows an F-distribution, and its p-value tells us if the predictors are collectively significant."],"metadata":{"id":"KXrUxyKBBqxQ"}},{"cell_type":"markdown","source":["### Example: Multiple Linear Regression with Sales and Advertising Data\n","\n","Let’s consider advertising data again. Assume we performed a multiple linear regression analysis and computed the F-statistic to test if at least one of the predictors is related to sales.\n","\n","\n","| Coefficient | Estimate | Std. Error | p-Value |\n","|-------------|----------|------------|---------|\n","| Intercept    | 2.939    | 0.3119     | <0.0001 |\n","| TV           | 0.046    | 0.0014     | <0.0001 |\n","| Radio        | 0.189    | 0.0086     | <0.0001 |\n","| Newspaper    | -0.001   | 0.0059     | 0.8599  |\n","\n","| Quantity                 | Value  |\n","|--------------------------|--------|\n","| F-statistic              | 570    |\n","| Residual standard error  | 1.69   |\n","| R²                       | 0.897  |\n","\n","\n","__Interpretation__\n","\n","- F-statistic= 570: Since this is far larger than 1, it provides compelling evidence against the null hypothesis $H_0$. This suggests that at least one of the predictors is significantly related to sales.\n","- p-values: If very small (close to 0), it provides strong evidence against the null hypothesis, indicating that the predictors collectively have a significant relationship with the response.\n"],"metadata":{"id":"LJZqUZ2pESEy"}},{"cell_type":"markdown","source":["## 2. Feature Selection"],"metadata":{"id":"mdejVVjWZCXq"}},{"cell_type":"markdown","source":["\n","\n","Sometimes, we want to test if a subset of predictors is zero. Suppose we want to test if a particular subset of $q$ coefficients are zero:\n","\n","$$\n","H_0: \\beta_{p-q+1} = \\beta_{p-q+2} = \\cdots = \\beta_p = 0\n","$$\n","\n","We then fit a reduced model excluding these predictors and calculate the F-statistic to assess whether omitting these predictors significantly worsens the model fit.\n","\n","This helps with deciding on which predictors are more important. In fact, the most direct approach is to called the __all subsets or best subsets regression__: compute the least square fit for all subsets and choose the one that balances the training error with model size.\n","\n","However, if $p$ is large, the number of all substes could be huge $2^p$. For example, if $p=40$, the number of subsets is over a bilion. We discuss two approaches to address this issue:\n","\n","\n"],"metadata":{"id":"m53dpGklSt_l"}},{"cell_type":"markdown","source":["### Forward Selection\n","\n","- Begin with the null model— a model that contains an intercept but no predictors.\n","\n","- Fit $p$ simple linear regressions and add to the null model the variable that results in the lowest RSS.\n","\n","- Add to that model the variable that results in the lowest RSS for the new two-variable model.\n","\n","- Continue until some stopping rule is satisfied. For example, when all remaining variables have a p-value above a threshhold.  "],"metadata":{"id":"eby13qX8UH7s"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8oIHpTlHqUkt"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["### Backward Selection\n","\n","- Start with all variables in the model.\n","\n","- Remove the variable with the largest p-value—that is, the variable that is the least statistically significant.\n","\n","-  The new $(p − 1)-variable model$ is fit, and the variable with the largest p-value is removed.\n","\n","- This procedure continues until a stopping rule is reached. For instance, we may stop when all remaining variables have a p-value below some threshold."],"metadata":{"id":"ksxBtyw7VdkZ"}},{"cell_type":"code","source":[],"metadata":{"id":"R_c0q_e-WCyd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Model Fit:"],"metadata":{"id":"IQU_83keY6KI"}},{"cell_type":"markdown","source":["\n","\n","Two common numerical measures of model fit are:\n","- **Residual Standard Error (RSE)**\n","- **$ R^2$**: The fraction of variance explained by the model."],"metadata":{"id":"Z3_YcTvAWDLo"}},{"cell_type":"markdown","source":["\n","- In simple regression, $ R^2 $ is the square of the correlation between the response and the predictor.\n","- In multiple regression, $ R^2 $ is the square of the correlation between the response and the predicted values. In fact it is equal to ⁉\n","\n","$$\n","Cor(Y, \\hat{Y}\\ )^2\n","$$\n","\n","A high $ R^2 $ value (close to 1) indicates that the model explains a large portion of the variance in the response variable."],"metadata":{"id":"1pxd6saBXAuN"}},{"cell_type":"markdown","source":["**Example**:\n","- **Full Model**: For the Advertising data, regressing sales on TV, radio, and newspaper gives an $ R^2 $ of 0.8972.\n","- **Reduced Model**: Using only TV and radio gives an $ R^2 $ of 0.89719.\n","\n","Including newspaper barely increases $ R^2 $, suggesting it doesn't significantly improve the model. This is evident from the non-significant p-value for newspaper advertising.\n","\n","**Adding Predictors**:\n","- $ R^2 $ always increases when adding more predictors, even if they're weakly associated with the response.\n","- A small increase in $ R^2 $ indicates that the new predictor doesn't add much value.\n","\n","**Comparisons**:\n","- **TV Only**: $ R^2 = 0.61 $\n","- **TV and Radio**: $ R^2 = 0.89719 $\n","\n","Adding radio significantly improves $ R^2 $, showing that radio is an important predictor.\n","\n","**Residual Standard Error (RSE)**:\n","- **TV Only**: $ RSE = 3.26 $\n","- **TV and Radio**: $ RSE = 1.681 $\n","- **TV, Radio, and Newspaper**: $ RSE = 1.686 $\n","\n","Including newspaper doesn't reduce RSE, reinforcing that it's not a useful predictor.\n","\n","**Graphical Analysis**:\n","- Plotting data can reveal problems not visible in numerical statistics.\n","- A 3D plot of TV and radio vs. sales shows non-linear patterns, suggesting interaction effects between TV and radio.\n","\n","**Conclusion**:\n","- TV and radio are better predictors of sales than newspaper.\n","- The model should focus on TV and radio spending to predict sales accurately."],"metadata":{"id":"N3l-ELVBYzuB"}},{"cell_type":"markdown","source":["## 4. Predictions:"],"metadata":{"id":"QpQSxQ-ZY299"}},{"cell_type":"markdown","source":["\n","\n","### Simplified Explanation of Predictions in Multiple Regression\n","\n","**Predictions**:\n","\n","Once we have fit the multiple regression model, predicting the response $Y$ based on the values of the predictors $X_1, X_2, \\ldots, X_p$ is straightforward using the equation:\n","\n","$$ \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\cdots + \\hat{\\beta}_p X_p $$\n","\n","However, there are three types of uncertainty in these predictions:\n","\n","1. **Inaccuracy in Coefficient Estimates**:\n","   - The estimated coefficients $\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p$ are not the true population coefficients $\\beta_0, \\beta_1, \\ldots, \\beta_p$.\n","   - This inaccuracy is related to the **reducible error**.\n","   - We can compute a **confidence interval** to determine how close $\\hat{Y}$ is to the true regression function $f(X)$.\n","\n","2. **Model Bias**:\n","   - The linear model we assume for $f(X)$ is an approximation of reality, which introduces **model bias**.\n","   - We estimate the best linear approximation to the true function, but this explanation ignores the discrepancy and assumes the linear model is correct.\n","\n","3. **Random Error ($\\epsilon$)**:\n","   - Even if we knew the true coefficients, predictions cannot be perfect due to random error ($\\epsilon$) in the model.\n","   - This is the **irreducible error**.\n","   - **Prediction intervals** help quantify this uncertainty and are always wider than confidence intervals because they account for both reducible and irreducible errors."],"metadata":{"id":"zlT8XfyHYb9Z"}},{"cell_type":"markdown","source":["**Example**:\n","\n","For the Advertising data:\n","\n","- **Confidence Interval**:\n","  - Quantifies uncertainty about the average sales over many cities.\n","  - Given $100,000 spent on TV and $20,000 on radio, the 95% confidence interval for the average sales is $[10,985, 11,528]$.\n","  - This means 95% of such intervals will contain the true average value of $f(X)$.\n","\n","- **Prediction Interval**:\n","  - Quantifies uncertainty about sales for a particular city.\n","  - Given $100,000 spent on TV and $20,000 on radio in a specific city, the 95% prediction interval for sales is $[7,930, 14,580]$.\n","  - This means 95% of such intervals will contain the true sales value for that city.\n","\n","**Key Points**:\n","\n","- Both intervals are centered at 11,256, but the prediction interval is wider due to increased uncertainty about individual sales compared to average sales.\n","- Confidence intervals are narrower because they only account for the reducible error, while prediction intervals account for both reducible and irreducible errors."],"metadata":{"id":"sEX87FOUYwCS"}},{"cell_type":"code","source":[],"metadata":{"id":"5pfkJxnOYvc7"},"execution_count":null,"outputs":[]}]}