{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "90405c4a",
      "metadata": {
        "id": "90405c4a"
      },
      "source": [
        "#Multiclass classification using tenswrflow\n",
        "\n",
        "This Notebook serves as a step-by-step implementation for Multi-Class Classification using neural networks.\n",
        "\n",
        "Here is thge step by step guide:\n",
        "\n",
        "1. Load the Data: Read the Mints dataset into a DataFrame (e.g., using pandas).\n",
        "\n",
        "2. Preprocess: Clean and preprocess the data as needed (handling missing values, encoding categorical variables, etc.).\n",
        "\n",
        "3. Split the Data: Use train-test split to separate the data into training and testing sets.\n",
        "\n",
        "4. Select a Model: Choose an appropriate classification model from one of the libraries.\n",
        "\n",
        "5. Train the Model: Fit the model to your training data.\n",
        "\n",
        "6. Evaluate the Model: Use accuracy or other metrics to evaluate the model's performance on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c4160ed",
      "metadata": {
        "id": "6c4160ed"
      },
      "source": [
        "## The MNIST data set\n",
        "\n",
        "In our neural network lectures we will be using the MNIST (Modified National Institute of Standards and Technology) data set, comprising pixelated images of handwritten digits from 0 to 9. Each image consists of grayscale pixels ranging from 0 (no marking) to 255 (darkest marking), measuring handwriting intensity. Originally, it held 60,000 training images and 10,000 test images. More background information on the data set can be found at it's Wikipedia page https://en.wikipedia.org/wiki/MNIST_database. Throughout our neural network lectures, we'll engage with a smaller, lower-resolution `sklearn` version. This sklearn version offers reduced image quality for quicker data loading and algorithm fitting. You can access it using the load_digits function from the datasets module [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html).\n",
        "sklearn version\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if using a local computer you may need to install tensorflow\n",
        "\n",
        "#pip install tensorflow"
      ],
      "metadata": {
        "id": "sauEzvryZwbJ"
      },
      "id": "sauEzvryZwbJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "\n",
        "\n",
        "# Display the shape of the dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "B9DU9LbCQl1M"
      },
      "id": "B9DU9LbCQl1M",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2941e833",
      "metadata": {
        "id": "2941e833"
      },
      "outputs": [],
      "source": [
        "# Normalize the features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "813933c2",
      "metadata": {
        "id": "813933c2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Initialize the model: a sequential model is a linear stack of layers.\n",
        "## It allows us to build a neural network layer by layer,\n",
        "## where each layer has exactly one input tensor (high dim matrix) and one output tensor.\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer with one hidden layer\n",
        "## Add a Dense layer with 64 neurons, using ReLU activation.\n",
        "## 'input_shape' defines the shape of the input data, which is the number of features.\n",
        "\n",
        "#model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "\n",
        "# Output layer\n",
        "## Add a Dense layer with 10 neurons for output, using softmax activation.\n",
        "## This layer corresponds to the 10 classes (digits 0-9) for classification.\n",
        "model.add(Dense(10, activation='relu'))\n",
        "\n",
        "# Compile the model\n",
        "## Adam (Adaptive Moment Estimation) adjusts the weights (W, b) during training\n",
        "## to minimize the loss function.\n",
        "## Sparse categorical crossentropy is used as the loss function for multi-class classification.\n",
        "\n",
        "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "RyvUXGWqP_yN"
      },
      "id": "RyvUXGWqP_yN",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use model.fit to fit the model. You need the followin parameters:\n",
        "## train data, train labels,\n",
        "## epochs: the number of times the model will iterate over the training dataset.\n",
        "## batch_size: the number of training samples to be used in one iteration of model training.\n",
        "## Smaller bactch size leads to more frequent weight updates, larger\n",
        "## validation_split: the fraction of the training data to be set aside as validation data during training.\n",
        "## it helps to detect overfitting or underfitting. We discuss this later\n",
        "\n",
        "model.fit(train_data, train_label, epochs=50, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "vnaTxe4KQFJE"
      },
      "id": "vnaTxe4KQFJE",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use model.evaluate to print test loss and test accuracy.\n",
        "## discuss what they are.\n",
        "#test_loss, test_accuracy = model.evaluate(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "0p6CbvyuQYF4"
      },
      "id": "0p6CbvyuQYF4",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## find the predictions\n",
        "\n",
        "#predictions = model.predict(X_test)\n",
        "\n",
        "\n",
        "## display the predictions\n",
        "\n",
        "## Convert probabilities to class labels\n",
        "\n",
        "# predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "## display the predicted classes\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2yaLcDKEQa3B"
      },
      "id": "2yaLcDKEQa3B",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "In this multi-class classification setup, we used a relu activation function in the output layer to handle multiple classes. There are other non-linear functions we could use such as softmax (look that up).\n",
        "\n",
        "The loss function sparse_categorical_crossentropy is used to compute the loss for multi-class classification problems, where the target variable consists of integer labels. We could use other methods, look it up to get familiar with other methods.\n",
        "\n",
        "We can adjust the number of hidden units, learning rate, epochs, and batch size based on our performance needs. Read more here:\n",
        "\n",
        "https://medium.com/@sanjay_dutta/mastering-hyperparameters-learning-rate-batch-size-and-more-e3b4df6624dc"
      ],
      "metadata": {
        "id": "DvyZTuP7uk2Q"
      },
      "id": "DvyZTuP7uk2Q"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RMUxPkQRRIv4"
      },
      "id": "RMUxPkQRRIv4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}