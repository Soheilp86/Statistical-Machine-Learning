{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis (PCA)\n",
        "\n",
        "In this lecture, we will cover:\n",
        "\n",
        "1. What PCA is,\n",
        "2. How to compute and interpret it."
      ],
      "metadata": {
        "id": "tyMdIif6NITD"
      },
      "id": "tyMdIif6NITD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principal Components Analysis (PCA)\n"
      ],
      "metadata": {
        "id": "hckpfiUzoRvN"
      },
      "id": "hckpfiUzoRvN"
    },
    {
      "cell_type": "markdown",
      "id": "f38cec18",
      "metadata": {
        "id": "f38cec18"
      },
      "source": [
        "\n",
        "\n",
        "__How do we visualize high dimentional dataset?__\n",
        "\n",
        "Suppose that we wish to visualize $n$ observations with measurements on a set of p features, $X_1,X_2, . . . ,X_p$, as part of an exploratory data analysis. In otherwords, we have a collection of $n$ vectors in $p$ dimentional space $\\mathbb{R}^p$. The idea is that not all of these dimensions are equally interesting.\n",
        "\n",
        "PCA finds a small number of dimensions that are as interesting as possible. Here interesting is measured by the amount that the observations vary along each dimension. This yields a lower-dimensional and hopefully more informative representation, making PCA a powerful tool for data analysis and dimensionality reduction.\n",
        "\n",
        "Next section explains how these dimensions,\n",
        "or __principal components__, are found.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA-- Alinear Algebra Approach."
      ],
      "metadata": {
        "id": "cRPjxeLqoPc_"
      },
      "id": "cRPjxeLqoPc_"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d5FGDyujPQ9E"
      },
      "id": "d5FGDyujPQ9E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DrFmD88CoOeo"
      },
      "id": "DrFmD88CoOeo"
    },
    {
      "cell_type": "markdown",
      "id": "66d8e069",
      "metadata": {
        "id": "66d8e069"
      },
      "source": [
        "\n",
        "\n",
        "**An Informal Review of Singular Value Decomposition (SVD)**\n",
        "\n",
        "Consider an $n \\times p$ data matrix $\\mathbf{X} = [\\mathbf{x}_1, \\ldots, \\mathbf{x}_n]$, where each row represents a different data point (e.g., $n$ individuals), and each column represents one of $p$ features (e.g., height, weight, age, etc.).\n",
        "\n",
        "It is helpful to think of $\\mathbf{X}$ as a linear transformation $\\mathbf{X} : \\mathbb{R}^p \\to \\mathbb{R}^n$. Informally, this means $\\mathbf{X}$ takes in a vector $\\mathbf{v}$, and *scales* and *rotates* it ($\\mathbf{X}\\mathbf{v}$). For example:\n",
        "- Diagonal-like matrices **scale** components.\n",
        "- Orthogonal matrices **rotate** vectors.  \n",
        "\n",
        "SVD is a matrix factorization method that breaks the action of a matrix into these two operations. Formally, for any $n \\times p$ matrix $\\mathbf{X}$, there exist:\n",
        "- Orthogonal matrices $\\mathbf{U}_{n \\times n}$ and $\\mathbf{V}_{p \\times p}$, and\n",
        "- A diagonal-like matrix $\\mathbf{\\Sigma}_{n \\times p}$,  \n",
        "\n",
        "such that:\n",
        "$$\n",
        "\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T.\n",
        "$$\n",
        "\n",
        "We’ve previously seen that SVD can be used for tasks like image compression, utilizing the best $k$-rank approximation.\n",
        "\n",
        "---\n",
        "\n",
        " **Using SVD for PCA**\n",
        "\n",
        "**Step 1:** Mean Centering\n",
        "First, we assume the data is *mean-centered*. If necessary, replace each data point $\\mathbf{x}_i$ by:\n",
        "$$\n",
        "\\mathbf{x}_i - \\boldsymbol{\\mu}, \\quad \\text{where } \\boldsymbol{\\mu} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{x}_i\n",
        "$$\n",
        "is the column-wise average.\n",
        "\n",
        "---\n",
        "\n",
        "**Step 2:**\n",
        "Let $k \\leq p$ be the dimension of the lower-dimensional subspace we want. Our goal is to find points $\\mathbf{\\tilde{x}}_1, \\ldots, \\mathbf{\\tilde{x}}_n \\in \\mathbb{R}^p$ such that:\n",
        "1. They approximate $\\mathbf{x}_1, \\ldots, \\mathbf{x}_n$, respectively.\n",
        "2. They lie in a $k$-dimensional subspace of $\\mathbb{R}^p$.  \n",
        "\n",
        "The approximation minimizes the **reconstruction error**:\n",
        "$$\n",
        "\\sum_{i=1}^n \\|\\mathbf{x}_i - \\mathbf{\\tilde{x}}_i\\|^2.\n",
        "$$\n",
        "\n",
        "Alternatively, for the entire dataset, we minimize:\n",
        "$$\n",
        "\\|\\mathbf{X} - \\mathbf{\\tilde{X}}\\|_F^2,\n",
        "$$\n",
        "where $\\|\\cdot\\|_F$ is the Frobenius norm, and:\n",
        "$$\n",
        "\\mathbf{\\tilde{X}} = \\begin{bmatrix}\n",
        "\\mathbf{\\tilde{x}}_1 & \\cdots & \\mathbf{\\tilde{x}}_n\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**Step 3: Solution via Eckhart-Young Theorem**\n",
        "By the Eckhart-Young Theorem, the optimal $\\mathbf{\\tilde{X}}$ is given by the **best rank-$k$ approximation** of $\\mathbf{X}$, obtained from its SVD. Specifically:\n",
        "\n",
        "- Let $\\mathbf{U}_k$ be the $n \\times k$ matrix of the top $k$ left singular vectors of $\\mathbf{X}$.\n",
        "\n",
        "- Define $\\mathbf{Z} = \\mathbf{U}_k^T \\mathbf{X}$.  \n",
        "\n",
        "Then:\n",
        "$$\n",
        "\\mathbf{\\tilde{X}} = \\mathbf{U}_k \\mathbf{U}_k^T \\mathbf{X} = \\mathbf{U}_k \\mathbf{Z}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**Step 4:** Output of PCA\n",
        "The output of PCA consists of two matrices:\n",
        "\n",
        "1. **$\\mathbf{U}_k$:** Columns are the top $k$ left singular vectors (orthogonal basis for the $k$-dimensional subspace).\n",
        "2. **$\\mathbf{Z}$:** Coefficients for expressing each mean-centered data point $\\mathbf{x}_i$ as a linear combination of the top $k$ left singular vectors.\n",
        "\n",
        "In summary, PCA identifies the $k$-dimensional subspace that best approximates the original data while minimizing reconstruction error."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA 2: PCA, A Statistical Approach\n"
      ],
      "metadata": {
        "id": "VmWP18uZo4M6"
      },
      "id": "VmWP18uZo4M6"
    },
    {
      "cell_type": "markdown",
      "id": "7b637977",
      "metadata": {
        "id": "7b637977"
      },
      "source": [
        "\n",
        "**1. Variance and Covariance for Mean-Centered Data**\n",
        "\n",
        "Consider real data values $\\mathbf{x}$ with mean $\\bar{x}$ and $\\mathbf{y}$ with mean $\\bar{y}$. The **variance** measures how far the data are spread away from their mean, while the **covariance** measures the relationship between $\\mathbf{x}$ and $\\mathbf{y}$ values.\n",
        "\n",
        "---\n",
        "\n",
        "**(i) Variance**\n",
        "The variance of $\\mathbf{x}$, $Var(\\mathbf{x})$, and the variance of $\\mathbf{y}$, $Var(\\mathbf{y})$, are defined as:\n",
        "$$\n",
        "Var(\\mathbf{x}) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n}, \\quad\n",
        "Var(\\mathbf{y}) = \\frac{\\sum_{i=1}^n (y_i - \\bar{y})^2}{n}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**(ii) Covariance**\n",
        "The covariance of $\\mathbf{x}$ and $\\mathbf{y}$, $Cov(\\mathbf{x}, \\mathbf{y})$, is defined as:\n",
        "$$\n",
        "Cov(\\mathbf{x}, \\mathbf{y}) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n}.\n",
        "$$\n",
        "Key properties of covariance:\n",
        "- $Cov(\\mathbf{x}, \\mathbf{y}) = Cov(\\mathbf{y}, \\mathbf{x})$ (symmetry).\n",
        "- For **mean-centered data** (where $\\bar{x} = 0$ and $\\bar{y} = 0$), the formulas simplify to:\n",
        "  $$\n",
        "  Var(\\mathbf{x}) = \\frac{\\sum_{i=1}^n x_i^2}{n} = \\frac{1}{n} \\mathbf{x} \\cdot \\mathbf{x}, \\quad\n",
        "  Cov(\\mathbf{x}, \\mathbf{y}) = \\frac{\\sum_{i=1}^n x_i y_i}{n} = \\frac{1}{n} \\mathbf{x} \\cdot \\mathbf{y}.\n",
        "  $$\n",
        "\n",
        "---\n",
        " **2. Covariance Matrix**\n",
        "\n",
        "Consider an $n \\times m$ data matrix $\\mathbf{X}$, where the columns represent features, and assume that all columns have been mean-centered. The **covariance matrix**, $\\mathbf{A}$, is defined as:\n",
        "$$\n",
        "\\mathbf{A} = \\frac{1}{n} \\mathbf{X}^T \\mathbf{X}.\n",
        "$$\n",
        "\n",
        "---\n",
        "**Key Properties of the Covariance Matrix**\n",
        "1. **Symmetry**:  \n",
        "   Since $\\mathbf{A}^T = \\frac{1}{n} (\\mathbf{X}^T)^T \\mathbf{X}^T = \\frac{1}{n} \\mathbf{X}^T \\mathbf{X} = \\mathbf{A}$, the covariance matrix is symmetric.\n",
        "\n",
        "2. **Entries of $\\mathbf{A}$**:\n",
        "   - The diagonal entries, $\\mathbf{A}_{ii}$, represent the **variance** of the $i$th column of $\\mathbf{X}$.\n",
        "   - The off-diagonal entries, $\\mathbf{A}_{ij}$ ($i \\neq j$), represent the **covariance** between the $i$th and $j$th columns of $\\mathbf{X}$.\n",
        "\n",
        "---\n",
        "\n",
        "**PCA from Covariance Matrix**\n",
        "\n",
        "Revcall that PCA identifies principal components—orthogonal directions in the data—that maximize variance, capturing the most significant patterns and variability. The first principal component (PC1) explains the largest variance, with subsequent components (PC2, PC3, etc.) explaining the next largest variance.\n",
        "\n",
        "The covariance matrix captures the variance of each feature and the pairwise relationships between features. This structure serves as the foundation for PCA, where eigenvalues and eigenvectors of $\\mathbf{A}$ reveal the principal directions of variance in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1:\n",
        "Consider the data points $(-5,0)$, $(0,2)$, $(5,-2)$. Find the principle components of the matrix representing this data."
      ],
      "metadata": {
        "id": "-X2uH-mtooN_"
      },
      "id": "-X2uH-mtooN_"
    },
    {
      "cell_type": "markdown",
      "id": "9b58a6f4",
      "metadata": {
        "id": "9b58a6f4"
      },
      "source": [
        "\n",
        " The corresponding data matrix is\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathbf{X}=\n",
        "\\begin{pmatrix}\n",
        "   -5 & 0 \\\\\n",
        "0& 2 \\\\\n",
        "5&-2\n",
        "\\end{pmatrix},\n",
        "\\end{align*}.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data matrix\n",
        "X = np.array([[-5, 0],\n",
        "              [0, 2],\n",
        "              [5, -2]])"
      ],
      "metadata": {
        "id": "CAOA_8Qsjh4C"
      },
      "id": "CAOA_8Qsjh4C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Center the data (subtract the mean from each column)\n",
        "X_centered = X - np.mean(X, axis=0)\n",
        "\n",
        "# Compute the covariance matrix\n",
        "cov_matrix = np.cov(X_centered.T)\n",
        "\n",
        "# Compute the eigenvalues and eigenvectors\n",
        "eigvals, eigvecs = np.linalg.eig(cov_matrix)\n",
        "\n",
        "# Print results\n",
        "print(f\"Manual Eigenvalues: {eigvals}\")\n",
        "print(f\"Manual Eigenvectors: \\n{eigvecs}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHFaJlmPi_of",
        "outputId": "d9ff253e-a3c9-467a-d3f9-cc112c6f3a05"
      },
      "id": "PHFaJlmPi_of",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Eigenvalues: [26.12970335  2.87029665]\n",
            "Manual Eigenvectors: \n",
            "[[ 0.97541287  0.22038544]\n",
            " [-0.22038544  0.97541287]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Compute PCA using sklearn\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Print principal components and explained variance\n",
        "print(\"Sklearn PCA Results:\")\n",
        "print(f\"Explained Variance (Eigenvalues): {pca.explained_variance_}\")\n",
        "print(f\"Principal Components (Eigenvectors):\\n{pca.components_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEOjgnxqhio2",
        "outputId": "555030a5-4941-4f0a-9cd0-ceed8a2b97b7"
      },
      "id": "UEOjgnxqhio2",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sklearn PCA Results:\n",
            "Explained Variance (Eigenvalues): [26.12970335  2.87029665]\n",
            "Principal Components (Eigenvectors):\n",
            "[[ 0.97541287 -0.22038544]\n",
            " [ 0.22038544  0.97541287]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "717e3430",
      "metadata": {
        "id": "717e3430"
      },
      "source": [
        "## Example 2\n",
        "\n",
        "Use a principal component to determine on what direction should we project the points $(-1,3),(0,0),(1,-3)$  to maximize the variance on that line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d434e568",
      "metadata": {
        "id": "d434e568"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Step 1: Define the points\n",
        "\n",
        "# Step 2: Mean center the data\n",
        "\n",
        "# Step 3: Use PCA to find the principal component\n",
        "\n",
        "\n",
        "# The principal component direction\n",
        "\n",
        "# Step 4: Print the direction of the principal component (the line of maximum variance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 3\n",
        "$\\,$ Consider the $n\\times m = 4\\times 3$ data matrix $\\mathbf{X}$ defined as"
      ],
      "metadata": {
        "id": "i4jEiR-Qob33"
      },
      "id": "i4jEiR-Qob33"
    },
    {
      "cell_type": "markdown",
      "id": "6a5177bb",
      "metadata": {
        "id": "6a5177bb"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "$$\n",
        "\\scriptsize\n",
        "\\mathbf{X}=\n",
        "\\begin{pmatrix}\n",
        " 0 & 4 & 0 \\\\\n",
        " 2 & 0 &  1 \\\\\n",
        " -2 & 0 &  -1\\\\\n",
        "0 & -4 & 0\n",
        "\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "a) $\\,$ Without calculating eigenvectors, conjecture the first two principal components.\n",
        "\n",
        "b) $\\,$\n",
        " Check your answer to a) by computing the eigenvectors of the covariance\n",
        "matrix $\\mathbf{A}=\\frac{1}{n}\\mathbf{X}^T\\mathbf{X}.$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "921d6ac6",
      "metadata": {
        "id": "921d6ac6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jueptpPwoVxQ"
      },
      "id": "jueptpPwoVxQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application (Potential Final Topic)"
      ],
      "metadata": {
        "id": "BF7UOaLGoWkX"
      },
      "id": "BF7UOaLGoWkX"
    },
    {
      "cell_type": "markdown",
      "id": "7c8daddb",
      "metadata": {
        "id": "7c8daddb"
      },
      "source": [
        "\n",
        "\n",
        "1. **Facial Recognition Using PCA**\n",
        "\n",
        "Imagine we have a database of n faces. Each face is represented as a vector $x \\in \\mathbb{R}^m$, where $x_i$ signifies the intensity of the i-th pixel. The dimension $m$ can be extensive, but after applying PCA, we can accurately represent these faces using only a few hundred principal components.\n",
        "\n",
        "Now, let's define some key matrices:\n",
        "- $X$ is an $m \\times n$ matrix with columns representing the mean-centered faces in the database.\n",
        "- $U_k$ is an $n \\times k$ matrix with columns as the top k left singular vectors of $X$, known as \"eigenfaces.\"\n",
        "- $Z$ is an $n \\times k$ matrix, where each column $z_i$ represents the coefficients for expressing a face as a linear combination of eigenfaces.\n",
        "\n",
        "To identify the closest match for a new face $x$ in the database, we first calculate the coordinates of $x$ projected onto the subspace spanned by the eigenfaces. Then, we locate the nearest neighbor to this projection among the vectors $z_i$ for $i = 1, \\ldots, n$.\n",
        "\n",
        "\n",
        "2. **Latent Semantic Analysis**\n",
        "\n",
        "We want to group a collection of n documents based on their topics. Each document is represented as a vector in $\\mathbb{R}^m$, indicating the frequency of each keyword (the number of occurrences of the keyword divided by the total number of occurrences of all keywords).\n",
        "\n",
        "Let $x_1, \\ldots, x_n \\in \\mathbb{R}^m$ represent the mean-centered documents. One approach is to measure the similarity between two documents $x_i$ and $x_j$ using their inner product $x_i^\\top x_j$. However, this approach has limitations. It doesn't consider correlations among keywords; for instance, two different keywords like \"football\" and \"Premier League\" may be closely related but are treated as orthogonal.\n",
        "\n",
        "To address this, we apply PCA. We represent the i-th mean-centered document as a linear combination of the top k principal components, denoted by vector $z_i$. It is expected that the inner product $z_i^\\top z_j$ will provide a better measure of similarity between the i-th and j-th documents compared to $x_i^\\top x_j$. This considers keyword correlations and enhances the grouping of similar documents.\n",
        "\n",
        "\n",
        "3. **Kernel PCA**\n",
        "Kernel Principal Component Analysis (Kernel PCA) is an extension of Principal Component Analysis (PCA) that allows for nonlinear dimensionality reduction. PCA is a linear technique that works well for datasets with linear relationships between variables. However, it may not be effective when the relationships between variables are nonlinear. Kernel PCA addresses this limitation by mapping data into a higher-dimensional space where PCA can be applied effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f5458be",
      "metadata": {
        "id": "4f5458be"
      },
      "source": [
        "Refrences:\n",
        "    \n",
        "    1- https://www.statlearning.com/\n",
        "    \n",
        "    2- https://timothyprojectgig.github.io/JB_Math_Textbook/Advanced/LinearAlgebra/PCA/jnb3.html\n",
        "    \n",
        "    3- https://www.cs.ox.ac.uk/people/james.worrell/SVD-thin.pdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vOgh-sGBmDGs"
      },
      "id": "vOgh-sGBmDGs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HsOYIwgHqMtu"
      },
      "id": "HsOYIwgHqMtu",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hckpfiUzoRvN",
        "cRPjxeLqoPc_",
        "VmWP18uZo4M6",
        "-X2uH-mtooN_",
        "717e3430",
        "i4jEiR-Qob33",
        "6a5177bb",
        "BF7UOaLGoWkX",
        "7c8daddb"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}